---
title: "Data access"
format: html
editor: visual
---

## Data access on UKB-RAP

```{r}
#| label: setup
library(dplyr)
library(vroom)
library(sparklyr)
library(DBI)
library(stringr)
library(glue)
```

### Extracting data via `dx extract_dataset`

We are going to extract the following items and collapse them and join them into a single dataset:

-   [Age at recruitment](https://biobank.ndph.ox.ac.uk/ukb/field.cgi?id=21022) (Field: 21022)
-   [Genetic Sex](https://biobank.ndph.ox.ac.uk/ukb/field.cgi?id=22001) (Field: 22001)
-   [Ever smoked](https://biobank.ndph.ox.ac.uk/ukb/field.cgi?id=20160) (Field: 20160)
-   [PRS for Age related Macular Degeneration](https://biobank.ndph.ox.ac.uk/ukb/field.cgi?id=26205) (Field: 26205)

As well as proteomics assay metadata:

-   [Data field 30900](https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=30900) - Number of proteins measured

-   [Data field 30901](https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=30901) - Plate used for sample run

-   [Data field 30902](https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=30902) - Well used for sample run

For all of these fields we will grab results from instance 0 which is the measurements at baseline.

First, we have extract the project id and record id to use the table exporter

```{r}
#| label: project-setup
# Project_id
project_id <- system("dx env", intern = TRUE)
project_id <- project_id[str_detect(project_id, "project")]
project_id <- str_replace(project_id, ".*project", "project")

# Record_id
record_id <-"record-GbgbXqjJxYp6jPgKk9v4Gb7q" 
# Project_record_id
project_record_id <- paste0(project_id, ":", record_id)

# Paths to database
database_path <- system("dx find data --class database", intern =TRUE)
app_substring <- str_extract(database_path, '(app\\d+_\\d+)')
database_substring <- str_extract(database_path, 'database-([A-Za-z0-9]+)') %>% 
  str_to_lower()  %>% 
  str_replace("database-", "database_")
database <- paste0(database_substring, "__", app_substring)

```

There are various ways to discover what the field names are called and to extract them. One way is to extract the data dictionary for the project:

```{r}
#| eval: false
# requires pandas
system("pip install pandas")
system(glue("dx extract_dataset {app_substring}.dataset -ddd --delimiter ','"))
```

And then look up the fields you are interested in. Since we have the field numbers already we can extract everything programmatically as follows:

```{r}
field_ids <- c(
"eid",
"p21022",  
"p22001", 
"p20160_i0",  
"p26205", 
"p30900_i0",
"p30901_i0",
"p30902_i0")
field_names <- c("eid", "age", "sex", "ever_smoked_at_baseline", 
                 "prs_amd", "num_proteins_measured", "plate_used", "well_used")
field_entities <- str_c("participant.", field_ids, collapse = ",")
extract_dataset_cmd <- glue("dx extract_dataset {project_record_id} --fields  {field_entities} --o 'pheno_data.csv'")
system(extract_dataset_cmd)
```

Now we can read it back into R with the following:

```{r}
pheno_data <- vroom("pheno_data.csv", col_names = field_names, skip = 1)
pheno_data
```

## Extracting data via Spark

If there are more than 30 entities you want to extract, DNANexus recommends connecting to their spark cluster, however the Rstudio doesn't connect to the spark cluster. This is left for posterity, or if you choose to try these notebooks in the Jupyter kernel.

```{r}
#| label: spark-setup
#| eval: false
# Connect to master node to orchestrates the analysis in spark
port <- Sys.getenv("SPARK_MASTER_PORT")
master <- paste("spark://master:", 8090, sep = "")
sc <- spark_connect(master)
```

```{r}
#| eval: false
# Olink tables within database
tables <- DBI::dbGetQuery(sc, paste0("SHOW TABLES IN ", database))
tables %>%
    filter(str_detect(tableName, "olink")) %>%
    pull(tableName)
```

```{r}
#| eval: false
# Instance 0
table_dataframes_i0 <- replicate(12, data.frame(matrix(ncol = 0, nrow = 0)), simplify = FALSE)

# Loop through each table name
for (i in 1:12) {
  # Construct the table name
  table_name <- paste0("olink_instance_0_00", sprintf("%02d", i))

  # Construct the SQL query
  query <- paste0("SELECT * FROM ", database, ".", table_name)

  # Execute the query and store the result in a dataframe
  table_dataframes_i0[[i]] <- sdf_sql(sc, query)
}

# Pivot long
instance_0_sdf <- reduce(table_dataframes_i0, left_join, by = "eid") %>%
  mutate(ins_index = 0) %>%
  pivot_longer(cols = -c(eid, ins_index), names_to = "protein_id", values_to = "result") %>%
  na.omit()
```

### Extracting data already in the project

There are two approaches to doing this either via `dx download` or through reading data directly via the mounted volume. `dx download` will save a local copy to your Rstudio working directory, so the paths required change depending on how you approach this.

```{r}
#| eval: false
dir("/mnt/project/proteomics")
```

Or using `dx download` which will we use now:

```{r}
system("dx ls proteomics")
system("dx download -r proteomics/")
```

We can read in the look up tables for aligning the protein names provided with their UniProt ID (in a helpful file called coding143 \<<https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=143>\>)

```{r}
olink_anno <- vroom("proteomics/coding143.tsv") %>% 
  tidyr::separate(meaning, into = c("symbol", "description"), sep = ";") %>% 
  mutate(protein_id = coding, 
         symbol = str_to_lower(symbol))
```

Assay-level results are also available. These are generic tab-separated datasets and are available via the resources section in [Category 1839](https://biobank.ndph.ox.ac.uk/showcase/label.cgi?id=1839), but in our case we have already uploaded them to our project.

-   **Assay:** Provides the lookup between an assay, its respective UniProt ID and the Olink Explore panel in which it is categorised.

-   **Assay version number:** Provides the version number for each assay per panel lot number.

-   **Batch number:** Provides the shipment batch number for each plate ID, allowing for correction of potential batch processing effects.

-   **Limit of detection**: Provides the instance-level limit of detection for each assay per shipment plate, allowing for filtering of sample results based on target protein detectability.

-   **Panel lot number** :Provides the processing lot number per assay panel within each shipment batch.

-   **Processing start date**: Provides the processing date for each shipment plate, broken down by assay panel.

```{r}
olink_assay <- vroom("proteomics/olink_assay.dat") %>% 
  mutate(Assay = str_to_lower(Assay))

olink_assay_version <- vroom("proteomics/olink_assay_version.dat") %>% 
  mutate(Assay = str_to_lower(Assay))

olink_batch_number <- vroom("proteomics/olink_batch_number.dat")

olink_limit_of_detection <- vroom("proteomics/olink_limit_of_detection.dat") %>% 
  mutate(Assay = str_to_lower(Assay)) %>% 
  filter(Instance == 0)

olink_panel_lot_number <- vroom("proteomics/olink_panel_lot_number.dat") %>% 

olink_processing_start_date <- vroom("proteomics/olink_processing_start_date.dat")
```

Now let's finally read in the first instance of olink assay which was the baseline assessment, there are two more sets of data corresponding imaging visits.

```{r}
olink_npx_baseline <- vroom("proteomics/olink_instance_0.csv")
```

### Join data and save results

There are several approaches we could take here, I'll take the convention of producing a SummarizedExperiment container for the data which would consist of three things:

-   assay data - the protein measurements as rows, samples as columns

-   row data or annotation data - the metadata about the protein i.e. their human readable elements

-   column data or pheno data - the data corresponding to the samples i.e. the phenotype data

Let's prepare the annotationo data first:

```{r}
anno <- olink_anno %>% 
  left_join(olink_assay, by = c("symbol" = "Assay")) %>% 
  as.data.frame()
rownames(anno) <- anno$protein_id

assay_batch_metadata <- 
  olink_limit_of_detection %>% 
  left_join(olink_assay, by = "Assay") %>% 
  left_join(olink_processing_start_date, by = c("PlateID", "Panel")) %>% 
  left_join(olink_batch_number, by = "PlateID") %>% 
  left_join(olink_panel_lot_number, by = c("Batch", "Panel")) %>% 
  left_join(olink_assay_version, by = c("Panel_Lot_Nr", "Assay")) 

col_data <- olink_npx_baseline %>% 
  select(eid) %>% 
  left_join(pheno_data, by = "eid") %>% 
  as.data.frame()
rownames(col_data) <- col_data$eid


assay_data <- olink_npx_baseline %>% 
  select(-eid) %>% 
  as.matrix() %>% 
  t()
rownames(assay_data) <- anno$protein_id
colnames(assay_data) <- col_data$eid

```

We could proceed to check / verify that data is properly aligned here but for now we can create the SummarisedExperiment object

```{r}
library(SummarizedExperiment)
se_npx <- SummarizedExperiment(
  assays = list(npx = assay_data), 
  rowData = anno, 
  colData = col_data)
se_npx

```

### Backing up your work

#### Saving our results

Let's save our table and SummarizedExperiment for future use, with the `qs` package. Note that this is like saving an 'rds' file, but is less storage intensive.

```{r}
library(qs)
qsave(se_npx, "olink_instance_0_se.qs")
qsave(assay_batch_metadata, "olink_assay_batch_metadata.qs")
system("dx upload --destination stuart/ *.qs")
```

#### Via version control at the console

This is mainly for versioning the code we have used to produce - make sure you don't commit any of the data you've created!

```{r}
#| eval: false
gert::git_add(...)
gert::git_commit(...)
gert::git_push(...)
```

### Using dx-backup-folder

```{r}
#| eval: false
system("dx-backup-folder")
# restore the folder with
system("dx-restore-folder")
```
